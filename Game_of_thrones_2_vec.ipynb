{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import codecs\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/itachi/Natural_language_processing/data/GOT_data/got1.txt',\n",
       " '/home/itachi/Natural_language_processing/data/GOT_data/got2.txt',\n",
       " '/home/itachi/Natural_language_processing/data/GOT_data/got3.txt',\n",
       " '/home/itachi/Natural_language_processing/data/GOT_data/got4.txt',\n",
       " '/home/itachi/Natural_language_processing/data/GOT_data/got5.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd() + '/' + \"data/GOT_data/\"\n",
    "book_filenames = sorted(glob.glob(path + '*.txt'))\n",
    "book_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading '/home/itachi/Natural_language_processing/data/GOT_data/got1.txt'...\n",
      "Corpus is now 1770659 characters long\n",
      "\n",
      "Reading '/home/itachi/Natural_language_processing/data/GOT_data/got2.txt'...\n",
      "Corpus is now 4071041 characters long\n",
      "\n",
      "Reading '/home/itachi/Natural_language_processing/data/GOT_data/got3.txt'...\n",
      "Corpus is now 6391405 characters long\n",
      "\n",
      "Reading '/home/itachi/Natural_language_processing/data/GOT_data/got4.txt'...\n",
      "Corpus is now 8107945 characters long\n",
      "\n",
      "Reading '/home/itachi/Natural_language_processing/data/GOT_data/got5.txt'...\n",
      "Corpus is now 9719485 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, 'r', 'utf-8')as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'This edition contains the complete text of the original hardcover edition.\\n\\nNOT ONE WORD HAS BEEN OMITTED.\\n\\nA CLASH OF KINGS\\n\\nA Bantam Spectra Book\\n\\nPU'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_raw[:151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\", \" \",raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heraldic crest by Virginia Norey.\n",
      "[u'Heraldic', u'crest', u'by', u'Virginia', u'Norey']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[5])\n",
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus containes 1,818,103 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus containes {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-08 22:44:53,232 : INFO : collecting all words and their counts\n",
      "2017-08-08 22:44:53,234 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-08-08 22:44:53,297 : INFO : PROGRESS: at sentence #10000, processed 140984 words, keeping 10280 word types\n",
      "2017-08-08 22:44:53,343 : INFO : PROGRESS: at sentence #20000, processed 279730 words, keeping 13558 word types\n",
      "2017-08-08 22:44:53,402 : INFO : PROGRESS: at sentence #30000, processed 420336 words, keeping 16598 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-08 22:44:53,458 : INFO : PROGRESS: at sentence #40000, processed 556581 words, keeping 18324 word types\n",
      "2017-08-08 22:44:53,515 : INFO : PROGRESS: at sentence #50000, processed 686247 words, keeping 19714 word types\n",
      "2017-08-08 22:44:53,572 : INFO : PROGRESS: at sentence #60000, processed 828497 words, keeping 21672 word types\n",
      "2017-08-08 22:44:53,633 : INFO : PROGRESS: at sentence #70000, processed 973830 words, keeping 23093 word types\n",
      "2017-08-08 22:44:53,690 : INFO : PROGRESS: at sentence #80000, processed 1114967 words, keeping 24252 word types\n",
      "2017-08-08 22:44:53,748 : INFO : PROGRESS: at sentence #90000, processed 1260481 words, keeping 26007 word types\n",
      "2017-08-08 22:44:53,798 : INFO : PROGRESS: at sentence #100000, processed 1393203 words, keeping 26884 word types\n",
      "2017-08-08 22:44:53,857 : INFO : PROGRESS: at sentence #110000, processed 1532150 words, keeping 27809 word types\n",
      "2017-08-08 22:44:53,918 : INFO : PROGRESS: at sentence #120000, processed 1680961 words, keeping 28486 word types\n",
      "2017-08-08 22:44:53,970 : INFO : collected 29026 word types from a corpus of 1818103 raw words and 128868 sentences\n",
      "2017-08-08 22:44:53,972 : INFO : Loading a fresh vocabulary\n",
      "2017-08-08 22:44:54,018 : INFO : min_count=20 retains 5996 unique words (20% of original 29026, drops 23030)\n",
      "2017-08-08 22:44:54,019 : INFO : min_count=20 leaves 1718869 word corpus (94% of original 1818103, drops 99234)\n",
      "2017-08-08 22:44:54,067 : INFO : deleting the raw counts dictionary of 29026 items\n",
      "2017-08-08 22:44:54,071 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2017-08-08 22:44:54,074 : INFO : downsampling leaves estimated 1310706 word corpus (76.3% of prior 1718869)\n",
      "2017-08-08 22:44:54,079 : INFO : estimated required memory for 5996 words and 300 dimensions: 17388400 bytes\n",
      "2017-08-08 22:44:54,123 : INFO : resetting layer weights\n",
      "2017-08-08 22:44:54,324 : INFO : training model with 4 workers on 5996 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-08-08 22:44:55,337 : INFO : PROGRESS: at 6.71% examples, 432077 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:44:56,347 : INFO : PROGRESS: at 13.57% examples, 440016 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:44:57,354 : INFO : PROGRESS: at 19.80% examples, 428899 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:44:58,394 : INFO : PROGRESS: at 26.12% examples, 420352 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-08 22:44:59,405 : INFO : PROGRESS: at 30.80% examples, 396166 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-08 22:45:00,420 : INFO : PROGRESS: at 36.56% examples, 390224 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:45:01,431 : INFO : PROGRESS: at 41.18% examples, 380194 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:45:02,443 : INFO : PROGRESS: at 48.26% examples, 388112 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:45:03,444 : INFO : PROGRESS: at 52.97% examples, 380194 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:45:04,448 : INFO : PROGRESS: at 59.57% examples, 385649 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:45:05,469 : INFO : PROGRESS: at 64.72% examples, 380742 words/s, in_qsize 8, out_qsize 0\n",
      "2017-08-08 22:45:06,478 : INFO : PROGRESS: at 69.06% examples, 371723 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:45:07,485 : INFO : PROGRESS: at 75.10% examples, 373287 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:45:08,493 : INFO : PROGRESS: at 80.92% examples, 374602 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:45:09,498 : INFO : PROGRESS: at 87.99% examples, 379411 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:45:10,505 : INFO : PROGRESS: at 93.90% examples, 380163 words/s, in_qsize 7, out_qsize 0\n",
      "2017-08-08 22:45:11,406 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-08-08 22:45:11,409 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-08-08 22:45:11,424 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-08-08 22:45:11,428 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-08-08 22:45:11,432 : INFO : training on 9090515 raw words (6553470 effective words) took 17.1s, 383278 effective words/s\n",
      "2017-08-08 22:45:11,439 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300                              # Word vector dimensionality                      \n",
    "min_word_count = 20                             # Minimum word count                        \n",
    "num_workers = multiprocessing.cpu_count()       # Number of threads to run in parallel\n",
    "context = 10                                    # Context window size                                                                                    \n",
    "downsampling = 1e-3                             # Downsample setting for frequent words\n",
    "seed = 1\n",
    "# Train the model (this will take some time)\n",
    "print (\"Training model...\")\n",
    "Game_of_thrones_2_vec = w2v.Word2Vec(sentences, \n",
    "                                     workers=num_workers, \n",
    "                                     size=num_features, \n",
    "                                     min_count = min_word_count, \n",
    "                                     window = context, \n",
    "                                     sample = downsampling,\n",
    "                                     seed = seed\n",
    "                                    )\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "Game_of_thrones_2_vec.init_sims(replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 5996\n",
      "Word2Vec word vector length: 300\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(Game_of_thrones_2_vec.wv.vocab))\n",
    "print(\"Word2Vec word vector length:\",Game_of_thrones_2_vec[\"Stark\"].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-08 22:45:38,651 : INFO : saving Word2Vec object under trained/Game_of_thrones_2_vec.w2v, separately None\n",
      "2017-08-08 22:45:38,654 : INFO : not storing attribute syn0norm\n",
      "2017-08-08 22:45:38,656 : INFO : not storing attribute cum_table\n",
      "2017-08-08 22:45:38,827 : INFO : saved trained/Game_of_thrones_2_vec.w2v\n"
     ]
    }
   ],
   "source": [
    "Game_of_thrones_2_vec.save(os.path.join(\"trained\", \"Game_of_thrones_2_vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-08 22:45:40,200 : INFO : loading Word2Vec object from trained/Game_of_thrones_2_vec.w2v\n",
      "2017-08-08 22:45:40,342 : INFO : loading wv recursively from trained/Game_of_thrones_2_vec.w2v.wv.* with mmap=None\n",
      "2017-08-08 22:45:40,343 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-08 22:45:40,344 : INFO : setting ignored attribute cum_table to None\n",
      "2017-08-08 22:45:40,345 : INFO : loaded trained/Game_of_thrones_2_vec.w2v\n"
     ]
    }
   ],
   "source": [
    "Game_of_thrones_2_vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"Game_of_thrones_2_vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization Word2Vec model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)  \n",
    "#\n",
    "#all_word_vectors_matrix = Game_of_thrones_2_vec.wv.syn0\n",
    "#\n",
    "#all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)\n",
    "#\n",
    "#points = pd.DataFrame(\n",
    "#    [\n",
    "#        (word, coords[0], coords[1])\n",
    "#        for word, coords in [\n",
    "#            (word, all_word_vectors_matrix_2d[Game_of_thrones_2_vec.wv.vocab[word].index])\n",
    "#            for word in Game_of_thrones_2_vec.wv.vocab\n",
    "#        ]\n",
    "#    ],\n",
    "#    columns=[\"word\", \"x\", \"y\"]\n",
    "#)\n",
    "#\n",
    "#print(points.head(10))\n",
    "#sns.set_context(\"poster\")\n",
    "#points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))\n",
    "#\n",
    "#def plot_region(x_bounds, y_bounds):\n",
    "#    slice = points[\n",
    "#        (x_bounds[0] <= points.x) &\n",
    "#        (points.x <= x_bounds[1]) &\n",
    "#        (y_bounds[0] <= points.y) &\n",
    "#        (points.y <= y_bounds[1]) \n",
    "#    ]\n",
    "#    \n",
    "#    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n",
    "#    for i, points in slice.iterrows():\n",
    "#        ax.text(points.x + 0.005, points.y + 0.005, point.word, fontsize=11)\n",
    "#\n",
    "#plot_region(x_bounds = (4.0, 4.2), y_bounds = (-0.5, -0.1))\n",
    "#plot_region(x_bounds = (0, 1), y_bounds = (4, 4.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-08 22:45:43,621 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Mad', 0.8702609539031982),\n",
       " ('Rhaegar', 0.859840989112854),\n",
       " ('Aegon', 0.8517005443572998),\n",
       " ('Robert', 0.8361358046531677),\n",
       " ('Daeron', 0.8048063516616821),\n",
       " ('Jaehaerys', 0.7642648220062256),\n",
       " ('Justice', 0.7641534805297852),\n",
       " ('Hand', 0.7634960412979126),\n",
       " ('Doran', 0.7401446104049683),\n",
       " ('North', 0.737543523311615)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Game_of_thrones_2_vec.most_similar(\"Aerys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Arryn', 0.7937906384468079),\n",
       " ('Brandon', 0.7730824947357178),\n",
       " ('Lady', 0.751340389251709),\n",
       " ('murdered', 0.7447934150695801),\n",
       " ('bastard', 0.6961814761161804),\n",
       " ('Winterfell', 0.6904762983322144),\n",
       " ('Robb', 0.6817637085914612),\n",
       " ('Tully', 0.6800894141197205),\n",
       " ('Young', 0.6777874231338501),\n",
       " ('Roose', 0.6703848242759705)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Game_of_thrones_2_vec.most_similar(\"Stark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
